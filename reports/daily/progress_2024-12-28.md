# Daily Progress Report - langchain-ai/langchain
Generated at: 2024-12-28 16:19:16
Period: 2024-12-27 08:19:06 to 2024-12-28 08:19:16

## Statistics
- Issues: 10 updates
- Pull Requests: 8 updates

## Issues
### #15548 - [WARNING:pypdf._reader:incorrect startxref pointer(3)](https://github.com/langchain-ai/langchain/issues/15548)
**Status:** 🔴 Closed  
**Author:** SouSingh  
**Created:** 2024-01-04 16:25:57  
**Updated:** 2024-12-28 04:24:26  
**Labels:** Ɑ: doc loader, 🤖:bug  

<details><summary>Description</summary>

### System Info

I try to load pdf in from langchain.document_loaders import PyPDFDirectoryLoader
got error this WARNING:pypdf._reader:incorrect startxref pointer(3)

from langchain.document_loaders import PyPDFDirectoryLoader
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFDirectoryLoader("/content/pdfs/Carina Lueschen Masterarbeit Ryan Trecartin (1).pdf")
pages = loader.load_and_split()

will return blank array and warning error

### Who can help?

@hwchase17 @agola11 @sbusso 

### Information

- [ ] The official example notebooks/scripts
- [X] My own modified scripts

### Related Components

- [ ] LLMs/Chat Models
- [X] Embedding Models
- [ ] Prompts / Prompt Templates / Prompt Selectors
- [ ] Output Parsers
- [ ] Document Loaders
- [ ] Vector Stores / Retrievers
- [ ] Memory
- [ ] Agents / Agent Executors
- [ ] Tools / Toolkits
- [ ] Chains
- [ ] Callbacks/Tracing
- [ ] Async

### Reproduction

just load the pdf which having images

### Expected behavior

output should be in array of pdf data
</details>

### #28562 - [NotImplementedError in RootListenersTracer.on_llm_end callback](https://github.com/langchain-ai/langchain/issues/28562)
**Status:** 🟢 Open  
**Author:** wangkun001  
**Created:** 2024-12-06 04:45:26  
**Updated:** 2024-12-28 03:56:35  
**Labels:** 🤖:bug  

<details><summary>Description</summary>

### Checked other resources

- [X] I added a very descriptive title to this issue.
- [X] I searched the LangChain documentation with the integrated search.
- [X] I used the GitHub search to find a similar question and didn't find it.
- [X] I am sure that this is a bug in LangChain rather than my code.
- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).

### Example Code

from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_ollama import ChatOllama

llm = ChatOllama(base_url="http://127.0.0.1:32101",
                 model="qwen2:latest",
                 temperature=0.8,
                 num_predict=1024,
                 )

store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]


with_message_history = RunnableWithMessageHistory(llm, get_session_history)
config = {"configurable": {"session_id": "abc2"}}
response = with_message_history.invoke(
    [("human", "编程.")],
    config=config,
)
print(response.content)

### Error Message and Stack Trace (if applicable)

当然，我很乐意帮助您解决编程问题。请告诉我具体需要帮助的内容：

1. 您正在使用哪种编程语言（例如：Python, Java, C++, JavaScript等）？
2. 需要实现的功能或项目类型是什么？
3. 遇到的具体问题或错误信息是什么？

比如如果您在学习 Python 并遇到了一些问题，您可以告诉我您在编写代码时遇到的特定错误、尝试运行但没有得到预期结果的部分代码或具体场景。这样我就能提供更具体的帮助和指导。

请提供更多详细信息以便我能更好地协助您！
NotImplementedError in RootListenersTracer.on_llm_end callback: NotImplementedError('Trying to load an object that doesn\'t implement serialization: {\'lc\': 1, \'type\': \'not_implemented\', \'id\': [\'ollama\', \'_types\', \'Message\'], \'repr\': "Message(role=\'assistant\', content=\'\', images=None, tool_calls=None)"}')


### Description

I'm trying to use langchain ChatOllama RunnableWithMessageHistory
I expect it to run normally
but An abnormality message has appeared
`NotImplementedError in RootListenersTracer.on_llm_end callback: NotImplementedError('Trying to load an object that doesn\'t implement serialization: {\'lc\': 1, \'type\': \'not_implemented\', \'id\': [\'ollama\', \'_types\', \'Message\'], \'repr\': "Message(role=\'assistant\', content=\'\', images=None, tool_calls=None)"}')`

### System Info

aiohappyeyeballs==2.4.4
aiohttp==3.11.9
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.6.2.post1
async-timeout==4.0.3
attrs==24.2.0
certifi==2024.8.30
charset-normalizer==3.4.0
distro==1.9.0
exceptiongroup==1.2.2
filelock==3.16.1
frozenlist==1.5.0
fsspec==2024.10.0
greenlet==3.1.1
h11==0.14.0
httpcore==1.0.7
httpx==0.27.2
huggingface-hub==0.26.3
idna==3.10
Jinja2==3.1.4
jiter==0.8.0
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.3.9
langchain-core==0.3.21
langchain-ollama==0.2.1
langchain-openai==0.2.11
langchain-text-splitters==0.3.2
langsmith==0.1.147
MarkupSafe==3.0.2
mpmath==1.3.0
multidict==6.1.0
networkx==3.4.2
numpy==1.26.4
ollama==0.4.2
openai==1.57.0
orjson==3.10.12
packaging==24.2
pandas==2.2.3
propcache==0.2.1
pydantic==2.10.3
pydantic_core==2.27.1
python-dateutil==2.9.0.post0
pytz==2024.2
PyYAML==6.0.2
regex==2024.11.6
requests==2.32.3
requests-toolbelt==1.0.0
safetensors==0.4.5
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.36
sympy==1.13.3
tenacity==9.0.0
tiktoken==0.8.0
tokenizers==0.20.3
torch==2.2.2
tqdm==4.67.1
transformers==4.46.3
typing_extensions==4.12.2
tzdata==2024.2
urllib3==2.2.3
yarl==1.18.3

</details>

### #28950 - [core,anthropic[patch]: fix with_structured_output typing](https://github.com/langchain-ai/langchain/pull/28950)
**Status:** 🟢 Open  
**Author:** baskaryan  
**Created:** 2024-12-28 03:28:41  
**Updated:** 2024-12-28 03:29:01  
**Labels:** 🤖:bug, size:S  

### #28948 - [TypeError: DocumentIntelligenceClientOperationsMixin.begin_analyze_document() missing 1 required positional argument: 'body' when trying to use the AzureAIDocumentIntelligenceLoader with the bytes_source parameter](https://github.com/langchain-ai/langchain/issues/28948)
**Status:** 🟢 Open  
**Author:** hiroci  
**Created:** 2024-12-28 01:10:18  
**Updated:** 2024-12-28 01:12:56  
**Labels:** 🤖:bug  

<details><summary>Description</summary>

### Checked other resources

- [X] I added a very descriptive title to this issue.
- [X] I searched the LangChain documentation with the integrated search.
- [X] I used the GitHub search to find a similar question and didn't find it.
- [X] I am sure that this is a bug in LangChain rather than my code.
- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).

### Example Code

This raises a TypeError missing 1 required positional argument: 'body' (trying to use the bytes_source parameter)

```python
endpoint = ""
key = ""
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, mode='single',
    bytes_source=b'%PDF-1.7\n...%',
)

loader.load()
```

Seems like the error is in the `parse_bytes` function of the file `/langchain_community/document_loaders/parsers/doc_intelligence.py, line 116`

all of the other parsers in this file do not specify the name for the second argument in self.client.begin_analyze_document

Example of working parser:

```python
def parse_url(self, url: str) -> Iterator[Document]:
      from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
    
      poller = self.client.begin_analyze_document(
          self.api_model,
          AnalyzeDocumentRequest(url_source=url),
          # content_type="application/octet-stream",
          output_content_format="markdown" if self.mode == "markdown" else "text",
      )
      result = poller.result()
...
```

Parser that does **NOT** work

```python

def parse_bytes(self, bytes_source: bytes) -> Iterator[Document]:
      from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
  
      poller = self.client.begin_analyze_document(
          self.api_model,
          analyze_request=AnalyzeDocumentRequest(bytes_source=bytes_source),
          # content_type="application/octet-stream",
          output_content_format="markdown" if self.mode == "markdown" else "text",
      )
```
The `parse_bytes` function does not work properly, the second parameter should be body=... instead of analyze_request or do not specify the name of the parameter at all


### Error Message and Stack Trace (if applicable)

File "/home/projects/intelligent_chat-be/server/routers/v1/conversation/file_loader.py", line 114, in _load_azure
    document = loader.load()
               ^^^^^^^^^^^^^
  File "/home/projects/intelligent_chat-be/.venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py", line 31, in load
    return list(self.lazy_load())
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/projects/intelligent_chat-be/.venv/lib/python3.12/site-packages/langchain_community/document_loaders/doc_intelligence.py", line 105, in lazy_load
    yield from self.parser.parse_bytes(self.bytes_source)
  File "/home/projects/intelligent_chat-be/.venv/lib/python3.12/site-packages/langchain_community/document_loaders/parsers/doc_intelligence.py", line 116, in parse_bytes
    poller = self.client.begin_analyze_document(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/projects/intelligent_chat-be/.venv/lib/python3.12/site-packages/azure/core/tracing/decorator.py", line 94, in wrapper_use_tracer
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
TypeError: DocumentIntelligenceClientOperationsMixin.begin_analyze_document() missing 1 required positional argument: 'body'

### Description

I'm trying to use the azure document intelligence loader from langchain to process a sequence of bytes

### System Info

System Information
------------------
> OS:  Linux
> OS Version:  #1 SMP Fri Mar 29 23:14:13 UTC 2024
> Python Version:  3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]

Package Information
-------------------
> langchain_core: 0.3.28
> langchain: 0.3.13
> langchain_community: 0.3.13
> langsmith: 0.2.4
> langchain_openai: 0.2.14
> langchain_qdrant: 0.2.0
> langchain_text_splitters: 0.3.4
> langgraph_sdk: 0.1.48

Optional packages not installed
-------------------------------
> langserve

Other Dependencies
------------------
> aiohttp: 3.11.11
> async-timeout: Installed. No version info available.
> dataclasses-json: 0.6.7
> fastembed: Installed. No version info available.
> httpx: 0.27.2
> httpx-sse: 0.4.0
> jsonpatch: 1.33
> langsmith-pyo3: Installed. No version info available.
> numpy: 2.1.2
> openai: 1.58.1
> orjson: 3.10.12
> packaging: 24.1
> pydantic: 2.9.2
> pydantic-settings: 2.6.1
> PyYAML: 6.0.2
> qdrant-client: 1.12.2
> requests: 2.32.3
> requests-toolbelt: 1.0.0
> SQLAlchemy: 2.0.36
> tenacity: 9.0.0
> tiktoken: 0.8.0
> typing-extensions: 4.12.
</details>

### #28947 - [feat(openai): Update with_structured_output default for OpenAI](https://github.com/langchain-ai/langchain/pull/28947)
**Status:** 🟢 Open  
**Author:** jacoblee93  
**Created:** 2024-12-27 23:35:57  
**Updated:** 2024-12-27 23:47:25  
**Labels:** size:XS  

<details><summary>Description</summary>

Should be accompanied by a minor bump

Should we also set `strict=True` by default?
</details>

### #28946 - [(Core): Developer support in `ChatPromptTemplate`](https://github.com/langchain-ai/langchain/pull/28946)
**Status:** 🟢 Open  
**Author:** keenborder786  
**Created:** 2024-12-27 21:41:56  
**Updated:** 2024-12-27 23:09:02  
**Labels:** size:L  

<details><summary>Description</summary>

- **Description:** Developer Role Added in `ChatPromptTemplate`
- **Issue:** #28944

</details>

### #28944 - [Add "developer" role support to ChatPromptTempkate](https://github.com/langchain-ai/langchain/issues/28944)
**Status:** 🟢 Open  
**Author:** baskaryan  
**Created:** 2024-12-27 15:42:36  
**Updated:** 2024-12-27 23:04:22  
**Labels:** Ɑ:  core  

<details><summary>Description</summary>

### Privileged issue

- [X] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.

### Issue Content

Add support for 
```python
ChatPromptTemplate([("developer", "...")])
```

</details>

### #28790 - [Tool call not working for Sonnet-3.5](https://github.com/langchain-ai/langchain/issues/28790)
**Status:** 🔴 Closed  
**Author:** HasnainKhanNiazi  
**Created:** 2024-12-18 15:47:37  
**Updated:** 2024-12-27 21:45:34  
**Labels:** 🤖:bug  

<details><summary>Description</summary>

### Checked other resources

- [X] This is a bug, not a usage question. For questions, please use GitHub Discussions.
- [X] I added a clear and detailed title that summarizes the issue.
- [X] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).
- [X] I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.

### Example Code

```python
@tool
def find_categories(user_query: str):
    """ 
    find_categories tool: Perform a search query to retrieve the top N categories based on the user query.
    Params: user_query: A string containing the user query.
    Returns: list: A list of retrieved categories and their attributes.
    """
    found_categories = find_relevant_categories(user_query)
    return found_categories

class find_categories_Input(BaseModel):
    user_query: str = Field(description="User search query to find the categories")

@tool("find_categories", args_schema=find_categories_Input, return_direct=False)
def find_categories(user_query: str):
    """ 
    find_categories tool: Perform a search query to retrieve the top N categories based on the user query.
    Params: user_query: A string containing the user query.
    Returns: list: A list of retrieved categories and their attributes.
    """
    found_categories = find_relevant_categories(user_query)

    return found_categories

model = ChatAnthropic(model='claude-3-5-sonnet-20240620', temperature=0.6, max_tokens=4096)
class AgentState(TypedDict):
    """The state of the agent."""

    messages: Annotated[Sequence[BaseMessage], add_messages]

model = model.bind_tools(TOOLS)
tools_by_name = {tool.name: tool for tool in TOOLS}

def tool_node(state: AgentState):
    outputs = []
    for tool_call in state["messages"][-1].tool_calls:
        tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=json.dumps(tool_result),
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}

def call_model(
    state: AgentState,
    config: RunnableConfig,
):
    system_prompt = SystemMessage(
        system_prompt_new
    )
    # print("Sending this msg to LLM:\n", [system_prompt] + state["messages"])
    response = model.invoke([system_prompt] + state["messages"], config)
    return {"messages": [response]}

def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return "end"
    else:
        return "continue"

# Define a new graph
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)

workflow.set_entry_point("agent")

# We now add a conditional edge
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "tools",
        "end": END,
    },
)

workflow.add_edge("tools", "agent")

async def run_graph(user_input: str, thread_id: str):
    async with AsyncConnectionPool(conninfo=os.getenv("DB_URI"), max_size=20, kwargs=connection_kwargs) as pool: # this has been updated
        checkpointer = AsyncPostgresSaver(pool)
        await checkpointer.setup()
        
        graph = workflow.compile(checkpointer=checkpointer)
        config = {"configurable": {"thread_id": thread_id}}
        async for event in graph.astream_events(
            {"messages": [HumanMessage(content=user_input)]},
            version = 'v2', stream_mode="values", config=config
        ):
            if "on_chat_model_stream" == event['event']:
                if len(event['data']["chunk"].content) > 0:
                    print(event['data']['chunk'].content, end='', flush=True)
```


### Error Message and Stack Trace (if applicable)

```shell
[{'id': 'toolu_01WX7gs7ALFqybEHQzDa5S5K', 'input': {}, 'name': 'find_categories', 'type': 'tool_use', 'index': 1}]
```


### Description

I defined a tool called `find_categories` in two different ways to test but in the case of `claude-3-5-sonnet-20240620` input is always empty. When I use OpenAI GPT4o, it works fine. What could be wrong?

### System Info

System Information
------------------
> OS:  Darwin
> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:11 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6020
> Python Version:  3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]

Package Information
-------------------
> langchain_core: 0.3.25
> langchain: 0.3.12
> langchain_community: 0.3.12
> langsmith: 0.1.145
> langchain_anthropic: 0.2.1
> langchain_experimental: 0.3.0
> langchain_google_genai: 2.0.3
> langchain_google_vertexai: 2.0.9
> langchain_openai: 0.2.12
> langchain_text_splitters: 0.3.3
> langchainhub: 0.1.20
> langgraph_sdk: 0.1.47
> langserve: 0.3.0

Other Dependencies
------------------
> aiohttp: 3.9.5
> anthropic: 0.34.2
> anthropic[vertexai]: Installed. No version info available.
> async-timeout: 4.0.3
> dataclasses-json: 0.6.7
> defusedxml: 0.7.1
> fastapi: 0.112.0
> google-cloud-aiplatform: 1.75.0
> google-cloud-storage: 2.19.0
> google-generativeai: 0.8.3
> httpx: 0.27.0
> httpx-sse: 0.4.0
> jsonpatch: 1.33
> langchain-mistralai: Installed. No version info available.
> numpy: 1.26.4
> openai: 1.57.2
> orjson: 3.10.6
> packaging: 24.1
> pillow: 10.4.0
> pydantic: 2.9.2
> pydantic-settings: 2.5.2
> PyYAML: 6.0.1
> requests: 2.32.3
> requests-toolbelt: 1.0.0
> SQLAlchemy: 2.0.31
> sse-starlette: 1.8.2
> tenacity: 8.5.0
> tiktoken: 0.7.0
> types-requests: 2.32.0.20240712
> typing-extensions: 4.12.2
</details>

### #28934 - [ChatOllama does not parse yfinance output correctly](https://github.com/langchain-ai/langchain/issues/28934)
**Status:** 🟢 Open  
**Author:** dlin95123  
**Created:** 2024-12-27 02:33:24  
**Updated:** 2024-12-27 21:13:13  
**Labels:** 🤖:bug, Ɑ:  models  

<details><summary>Description</summary>

### Checked other resources

- [X] I added a very descriptive title to this issue.
- [X] I searched the LangChain documentation with the integrated search.
- [X] I used the GitHub search to find a similar question and didn't find it.
- [X] I am sure that this is a bug in LangChain rather than my code.
- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).

### Example Code


from dotenv import load_dotenv
import os

load_dotenv()

from langchain_ollama import ChatOllama 
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough 
from langchain_core.prompts import ChatPromptTemplate

from langchain_core.tools import tool
import yfinance as yf

llm = ChatOllama(model='llama3.1', temperature=0)
#llm = ChatGroq(model='llama-3.1-8b-instant', temperature =0)
#llm = ChatOpenAI(model='gpt-4o-mini', temperature = 0)

from langchain_core.tools import tool, StructuredTool
from datetime import date

@tool
def company_information(ticker: str) -> dict:
    """Use this tool to retrieve company information like address, industry, sector, company officers, business summary, website,
       marketCap, current price, ebitda, total debt, total revenue, debt-to-equity, etc."""
    
    ticker_obj = yf.Ticker(ticker)
    ticker_info = ticker_obj.get_info()

    return ticker_info

@tool
def last_dividend_and_earnings_date(ticker: str) -> dict:
    """
    Use this tool to retrieve company's last dividend date and earnings release dates.
    It does not provide information about historical dividend yields.
    """
    ticker_obj = yf.Ticker(ticker)
    
    return ticker_obj.get_calendar()

@tool
def summary_of_mutual_fund_holders(ticker: str) -> dict:
    """
    Use this tool to retrieve company's top mutual fund holders. 
    It also returns their percentage of share, stock count and value of holdings.
    """
    ticker_obj = yf.Ticker(ticker)
    mf_holders = ticker_obj.get_mutualfund_holders()
    
    return mf_holders.to_dict(orient="records")

@tool
def summary_of_institutional_holders(ticker: str) -> dict:
    """
    Use this tool to retrieve company's top institutional holders. 
    It also returns their percentage of share, stock count and value of holdings.
    """
    ticker_obj = yf.Ticker(ticker)   
    inst_holders = ticker_obj.get_institutional_holders()
    
    return inst_holders.to_dict(orient="records")

@tool
def stock_grade_updrages_downgrades(ticker: str) -> dict:
    """
    Use this to retrieve grade ratings upgrades and downgrades details of particular stock.
    It'll provide name of firms along with 'To Grade' and 'From Grade' details. Grade date is also provided.
    """
    ticker_obj = yf.Ticker(ticker)
    
    curr_year = date.today().year
    
    upgrades_downgrades = ticker_obj.get_upgrades_downgrades()
    upgrades_downgrades = upgrades_downgrades.loc[upgrades_downgrades.index > f"{curr_year}-01-01"]
    upgrades_downgrades = upgrades_downgrades[upgrades_downgrades["Action"].isin(["up", "down"])]
    
    return upgrades_downgrades.to_dict(orient="records")

@tool
def stock_splits_history(ticker: str) -> dict:
    """
    Use this tool to retrieve company's historical stock splits data.
    """
    ticker_obj = yf.Ticker(ticker)
    hist_splits = ticker_obj.get_splits()
    
    return hist_splits.to_dict()

@tool
def stock_news(ticker: str) -> dict:
    """
    Use this to retrieve latest news articles discussing particular stock ticker.
    """
    ticker_obj = yf.Ticker(ticker)
    
    return ticker_obj.get_news()


from langchain.agents import AgentExecutor
from langchain.agents import create_tool_calling_agent

#from langchain.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder

tools = [
         company_information,
         last_dividend_and_earnings_date,
         stock_splits_history,
         summary_of_mutual_fund_holders,
         summary_of_institutional_holders, 
         stock_grade_updrages_downgrades,
         stock_news
]
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Try to answer user query using available tools. Parse the input carefully.",
        ),
        MessagesPlaceholder(variable_name="messages"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

agent = create_tool_calling_agent(llm, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

from langchain_core.messages import HumanMessage

resp = agent_executor.invoke({"messages": [HumanMessage(content="What is address of Nike?")]})


### Error Message and Stack Trace (if applicable)

_No response_

### Description

I am testing local LLMs with ChatOllama and using llama3.1 for answering output from yfinance. I found that it cannot answer simple question like "What is Nike's address?". It can answer longer questions okay, just not short question. 

It works pretty well with ChatOpenAI() and ChatGroq() with the same model. You can reproduce the issue with the attached code. 

### System Info

System Information
------------------
> OS:  Windows
> OS Version:  10.0.22631
> Python Version:  3.12.8 (tags/v3.12.8:2dc476b, Dec  3 2024, 19:30:04) [MSC v.1942 64 bit (AMD64)]

Package Information
-------------------
> langchain_core: 0.3.28
> langchain: 0.3.12
> langchain_community: 0.3.12
> langsmith: 0.2.3
> langchain_groq: 0.2.2
> langchain_huggingface: 0.1.0
> langchain_ollama: 0.2.1
> langchain_openai: 0.2.12
> langchain_text_splitters: 0.3.3
> langgraph_sdk: 0.1.45

Optional packages not installed
-------------------------------
> langserve

Other Dependencies
------------------
> aiohttp: 3.11.10
> async-timeout: Installed. No version info available.
> dataclasses-json: 0.6.7
> groq: 0.13.1
> httpx: 0.27.2
> httpx-sse: 0.4.0
> huggingface-hub: 0.25.1
> jsonpatch: 1.33
> langsmith-pyo3: Installed. No version info available.
> numpy: 1.26.4
> ollama: 0.4.4
> openai: 1.58.1
> orjson: 3.10.12
> packaging: 24.2
> pydantic: 2.9.2
> pydantic-settings: 2.7.0
> PyYAML: 6.0.2
> requests: 2.32.3
> requests-toolbelt: 1.0.0
> sentence-transformers: 3.3.1
> SQLAlchemy: 2.0.36
> tenacity: 9.0.0
> tiktoken: 0.8.0
> tokenizers: 0.21.0
> transformers: 4.47.0
> typing-extensions: 4.12.2
</details>

### #28943 - [max_completion_tokens (and max_tokens) param in ChatOpenAI() can't be processed by OpenAI() object](https://github.com/langchain-ai/langchain/issues/28943)
**Status:** 🟢 Open  
**Author:** Armasse  
**Created:** 2024-12-27 15:35:54  
**Updated:** 2024-12-27 20:51:54  
**Labels:** 🤖:bug, investigate  

<details><summary>Description</summary>

### Checked other resources

- [X] I added a very descriptive title to this issue.
- [X] I searched the LangChain documentation with the integrated search.
- [X] I used the GitHub search to find a similar question and didn't find it.
- [X] I am sure that this is a bug in LangChain rather than my code.
- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).

### Example Code

```python
from langchain_openai import ChatOpenAI

chat_model = ChatOpenAI(
    model_name="model",
    max_completion_tokens=800,
    openai_api_base="base_url",
    openai_api_key="your_key"
)

chat_model.invoke("Hello, how are you ?")
```

### Error Message and Stack Trace (if applicable)

```bash
Traceback (most recent call last):
  File "/home/user/test_python/langchain_bug.py", line 12, in <module>
    chat_model.invoke("Hello, how are you ?")
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 289, in invoke
    self.generate_prompt(
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 800, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 655, in generate
    raise e
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 645, in generate
    self._generate_with_cache(
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 872, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 726, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/openai/_utils/_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 859, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/openai/_base_client.py", line 1280, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/openai/_base_client.py", line 957, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/user/.cache/pypoetry/virtualenvs/test-python-OI3Fy4Nv-py3.12/lib/python3.12/site-packages/openai/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'object': 'error', 'message': "[{'type': 'extra_forbidden', 'loc': ('body', 'max_completion_tokens'), 'msg': 'Extra inputs are not permitted', 'input': 800}]", 'type': 'BadRequestError', 'param': None, 'code': 400}
```

### Description

I'm trying to use langchain ChatOpenAI() object with max_completion_tokens parameter initialized. Since September 2024, the max_tokens parameter is deprecated in favor of max_completion_tokens. The change was made in langchain but for now, it has not been done in the OpenAI Python library.

When I pass max_completion_tokens parameter, an error is raised because extra parameter is forbidden when we create OpenAI() object (from the OpenAI Python library).

I know, it's not a bug from the langchain library strictly speaking. But while waiting for the OpenAI library to make the change, is it possible to mitigate the problem? Because, for now, the feature is unavailable.

### System Info

System Information
------------------
> OS:  Linux
> OS Version: #1 SMP Tue Nov 5
> Python Version:  3.12.5
 
Package Information
-------------------
> langchain_core: 0.3.28
> langchain: 0.3.13
> langsmith: 0.2.6
> langchain_bug: Installed. No version info available.
> langchain_openai: 0.2.14
> langchain_text_splitters: 0.3.4
 
Optional packages not installed
-------------------------------
> langserve
 
Other Dependencies
------------------
> aiohttp: 3.11.11
> async-timeout: Installed. No version info available.
> httpx: 0.27.2
> httpx-sse: 0.4.0
> jsonpatch: 1.33
> langsmith-pyo3: Installed. No version info available.
> numpy: 2.2.1
> openai: 1.58.1
> orjson: 3.10.12
> packaging: 24.2
> pydantic: 2.10.4
> PyYAML: 6.0.2
> requests: 2.32.3
> requests-toolbelt: 1.0.0
> SQLAlchemy: 2.0.36
> tenacity: 9.0.0
> tiktoken: 0.8.0
> tokenizers: 0.21.0
> typing-extensions: 4.12.2
> zstandard: Installed. No version info available. 
</details>

## Pull Requests
### #28950 - [core,anthropic[patch]: fix with_structured_output typing](https://github.com/langchain-ai/langchain/pull/28950)
**Status:** 🟡 Open  
**Author:** baskaryan  
**Created:** 2024-12-28 03:28:41  
**Updated:** 2024-12-28 03:29:01  
**Branch:** `bagatur/fix_structured_output_typing` → `master`  

### #28947 - [feat(openai): Update with_structured_output default for OpenAI](https://github.com/langchain-ai/langchain/pull/28947)
**Status:** 🟡 Open  
**Author:** jacoblee93  
**Created:** 2024-12-27 23:35:57  
**Updated:** 2024-12-27 23:47:25  
**Branch:** `jacob/o1_structured_output` → `master`  

<details><summary>Description</summary>

Should be accompanied by a minor bump

Should we also set `strict=True` by default?
</details>

### #28946 - [(Core): Developer support in `ChatPromptTemplate`](https://github.com/langchain-ai/langchain/pull/28946)
**Status:** 🟡 Open  
**Author:** keenborder786  
**Created:** 2024-12-27 21:41:56  
**Updated:** 2024-12-27 23:09:02  
**Branch:** `developer_support` → `master`  

<details><summary>Description</summary>

- **Description:** Developer Role Added in `ChatPromptTemplate`
- **Issue:** #28944

</details>

### #28942 - [docs: Update documentation for BoxBlobLoader, extra_fields](https://github.com/langchain-ai/langchain/pull/28942)
**Status:** 🟢 Merged  
**Author:** shurrey  
**Created:** 2024-12-27 15:05:02  
**Updated:** 2024-12-27 20:06:58  
**Branch:** `box/blob_loader_docs` → `master`  

<details><summary>Description</summary>

Thank you for contributing to LangChain!

- [x] **PR title**: "package: description"
  - Where "package" is whichever of langchain, community, core, etc. is being modified. Use "docs: ..." for purely docs changes, "infra: ..." for CI changes.
  - Example: "community: add foobar LLM"


  - **Description:** Update docs to add BoxBlobLoader and extra_fields to all Box connectors.
  - **Issue:** N/A
  - **Dependencies:** N/A
  - **Twitter handle:** @BoxPlatform


- [x] **Add tests and docs**: If you're adding a new integration, please include
  1. a test for the integration, preferably unit tests that do not rely on network access,
  2. an example notebook showing its use. It lives in `docs/docs/integrations` directory.


- [x] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/

Additional guidelines:
- Make sure optional dependencies are imported within a function.
- Please do not add dependencies to pyproject.toml files (even optional ones) unless they are required for unit tests.
- Most PRs should not touch more than one package.
- Changes should be backwards compatible.
- If you are adding something to community, do not re-import it in langchain.

If no one reviews your PR within a few days, please @-mention one of baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.

</details>

### #28941 - [docs: add modelscope endpoint](https://github.com/langchain-ai/langchain/pull/28941)
**Status:** 🟡 Open  
**Author:** Yunnglin  
**Created:** 2024-12-27 09:39:39  
**Updated:** 2024-12-27 15:41:26  
**Branch:** `add/modelscope` → `master`  

<details><summary>Description</summary>

## Description

To integrate ModelScope inference API endpoints for both Embeddings, LLMs and ChatModels, install the package `langchain-modelscope-integration` (as discussed in issue #28928 ). This is necessary because the package name `langchain-modelscope` was already registered by another party.

ModelScope is a premier platform designed to connect model checkpoints with model applications. It provides the necessary infrastructure to share open models and promote model-centric development. For more information, visit GitHub page: [ModelScope](https://github.com/modelscope).
</details>

### #28914 - [docs: Add FalkorDB Chat Message History and Update Package Registry](https://github.com/langchain-ai/langchain/pull/28914)
**Status:** 🟡 Open  
**Author:** kingtroga  
**Created:** 2024-12-25 08:35:43  
**Updated:** 2024-12-27 14:05:25  
**Branch:** `falkordbmessagehistorycreationnewnew` → `master`  

<details><summary>Description</summary>

This commit updates the documentation and package registry for the FalkorDB Chat Message History integration.

**Changes:**

- Added a comprehensive example notebook falkordb_chat_message_history.ipynb demonstrating how to use FalkorDB for session-based chat message storage.

- Added a provider notebook for FalkorDB

- Updated libs/packages.yml to register FalkorDB as an integration package, following LangChain's new guidelines for community integrations.

**Notes:**

- This update aligns with LangChain's process for registering new integrations via documentation updates and package registry modifications.

- No functional or core package changes were made in this commit.
</details>

### #24994 - [community: Enhance Atlassian Integration with New API Wrapper and Toolkit](https://github.com/langchain-ai/langchain/pull/24994)
**Status:** 🔴 Closed  
**Author:** danielgines  
**Created:** 2024-08-02 19:23:13  
**Updated:** 2024-12-27 09:07:51  
**Branch:** `atlassian` → `master`  

<details><summary>Description</summary>

# Enhance Atlassian Integration with New API Wrapper and Toolkit

## Description
This pull request introduces significant enhancements to the Langchain project by integrating the new `AtlassianAPIWrapper` and `AtlassianAction` libraries. These updates replace the outdated `jira` library, providing a more robust and comprehensive solution for interacting with Atlassian's services, including Jira and Confluence.

### Key Enhancements
1. **Enhanced Jira Integration:**
   - Full support for Jira functions as provided by the latest Atlassian Python API.
   - Advanced error handling mechanisms to ensure reliable operation.
   - Fixes authentication issues for both Atlassian Cloud and Data Center.

2. **Comprehensive Confluence Integration:**
   - Full support for Confluence functions as provided by the latest Atlassian Python API.
   - Supports CQL queries and detailed function retrieval and execution.

3. **Improved Configuration and Inputs:**
   - Inputs for instance URLs, usernames, API tokens, and cloud configurations.
   - Support for filtering response keys to streamline the data handling process.

4. **Migration to Pydantic v2:**
   - Updated to use the latest version of Pydantic for data validation and management.
   - Ensures compatibility with the most recent developments in the Python ecosystem.

5. **Full Coverage of Atlassian Python API Functionalities:**
   - Provides coverage of functionalities for both Jira and Confluence, ensuring feature parity with the official Atlassian Python API.

6. **Error Handling and Authentication Improvements:**
   - Advanced error handling for better resilience and reliability.
   - Resolves previous issues with authentication for cloud and data center deployments.

### Additional Enhancements
1. **Docstrings and Documentation:**
   - Added detailed docstrings to all major functions in the `AtlassianAPIWrapper` module.
   - Improved code documentation to facilitate easier understanding and future development.

2. **Integration Tests:**
   - Comprehensive test suite to ensure the reliability and correctness of the `AtlassianAPIWrapper`.
   - Tests cover both Jira and Confluence functionalities provided by the Atlassian Python API.

3. **Formatting and Environment Checks:**
   - Applied lint formatting corrections to ensure compliance with standards.
   - Added checks for required environment variables before running tests.
   - Improved code readability and maintainability.

4. **Jupyter Notebook Guide:**
   - Added a comprehensive Jupyter notebook that serves as a guide for using the newly developed Atlassian Toolkit.
   - The notebook includes detailed examples for interacting with Jira and Confluence APIs, showcasing how to:
     - Set up necessary environment variables
     - Initialize the AtlassianAPIWrapper
     - Execute JQL queries in Jira
     - Retrieve and use Jira and Confluence functions
     - Apply filters to responses
     - Extend the module to support future Atlassian products
   - This addition aims to provide a helpful resource for developers to understand and utilize the module effectively in their projects.

For more details, users are directed to the [Atlassian Python API documentation](https://atlassian-python-api.readthedocs.io/).

### Issue:
[#20084](https://github.com/langchain-ai/langchain/issues/20084#issue-2228740501)

### Dependencies:
No additional dependencies required.

### Twitter handle:
Twitter handle: dgines

---

- [x] Added tests and docs for the new integration.
- [x] Ensured all changes are backward compatible.
- [x] Ran `make format`, `make lint`, and `make test` to ensure code quality.
</details>

### #28466 - [partners/pinecone: release 0.2.2](https://github.com/langchain-ai/langchain/pull/28466)
**Status:** 🟢 Merged  
**Author:** efriis  
**Created:** 2024-12-03 06:47:48  
**Updated:** 2024-12-27 08:52:46  
**Branch:** `erick/partners-pinecone-release-0-2-2` → `master`  

